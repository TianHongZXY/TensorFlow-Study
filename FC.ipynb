{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_3(num_units_x, num_units_L1, num_units_L2, num_units_y):\n",
    "    \"\"\"\n",
    "    初始化计算三层网络的weight和bias项\n",
    "    \"\"\"\n",
    "    \n",
    "    # weight的shape是(下一层的节点数，上一层的节点数)\n",
    "    np.random.seed(3)\n",
    "    W1 = np.random.randn(num_units_L1, num_units_x) * 0.1\n",
    "    b1 = np.zeros(shape=(num_units_L1, 1))\n",
    "    W2 = np.random.randn(num_units_L2, num_units_L1) * 0.1\n",
    "    b2 = np.zeros(shape=(num_units_L2, 1))\n",
    "    W3 = np.random.randn(num_units_y, num_units_L2) * 0.1\n",
    "    b3 = np.zeros(shape=(num_units_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_n(layer_dimentions):\n",
    "    \"\"\"\n",
    "    初始化可自定义神经网络层数的weight和bias\n",
    "    传入的layer_dimentions是列表，[1,2,3,4]代表第一层(即输入层X)节点数为1，第二层为2，以此类推，第4层(即输出层Y)为4\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    # 记录层数L\n",
    "    L = len(layer_dimentions)\n",
    "    parameters = {}\n",
    "    # 算上输入层共有L层，所以实际要计算的共L-1层，所以weight有L-1个\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dimentions[l], layer_dimentions[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dimentions[l], 1))\n",
    "        #print('b' + str(l)+ '.shape = ' + str(parameters['b' + str(l)].shape))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_relu(Z):\n",
    "    Z = np.maximum(0, Z)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.        ]\n",
      " [0.55469899 0.77661509 0.         0.        ]\n",
      " [0.         1.91201631 0.57451838 0.17176723]]\n"
     ]
    }
   ],
   "source": [
    "print(activation_relu(np.random.randn(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_sigmoid(Z):\n",
    "    Z = 1 / (1 + np.exp(-Z))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.93404033 0.27095508 0.21066288 0.38946652]\n",
      " [0.61655104 0.47455385 0.47659128 0.53819507]\n",
      " [0.8747573  0.1445259  0.75548924 0.2850665 ]]\n"
     ]
    }
   ],
   "source": [
    "print(activation_sigmoid(np.random.randn(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_tanh(Z):\n",
    "    Z = (np.exp(Z) - np.exp(-Z)) / (np.exp(Z) + np.exp(-Z))\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07292549 -0.9474426  -0.7188451   0.11214934]\n",
      " [-0.79475681 -0.95169994  0.96233135 -0.17953979]\n",
      " [ 0.11195175  0.22682011 -0.43707357  0.66465716]]\n"
     ]
    }
   ],
   "source": [
    "print(activation_tanh(np.random.randn(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    \"\"\"每一列为一个实例\"\"\"\n",
    "    #print(Z)\n",
    "    sum_e = np.sum(np.exp(Z),axis=0)\n",
    "    #print(sum_e)\n",
    "    Z = np.exp(Z) / sum_e\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.96699371 -1.2869314  -0.20105464  0.47178171]\n",
      " [-0.69329433  0.4430299  -0.92062036  0.27360391]\n",
      " [-0.6018457   1.15993733 -1.06568879 -0.79238439]]\n",
      "[8.19687782 5.02326903 1.56063012 3.37030537]\n",
      "[[0.87217986 0.05496755 0.52406251 0.4755793 ]\n",
      " [0.06098986 0.31004091 0.25519941 0.39008156]\n",
      " [0.06683028 0.63499154 0.22073808 0.13433915]]\n"
     ]
    }
   ],
   "source": [
    "print(softmax(np.random.randn(3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_prop(A_prev, W, b):\n",
    "    # A_prev是上一层计算得到的\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    # cache后面用作反向传播\n",
    "    cache = (A_prev, W, b, Z)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_prop(A_prev, W, b, activation):\n",
    "    \"\"\"含有激活函数的前向传播\"\"\"\n",
    "    #print('传入的b.shape = ' + str(b.shape))\n",
    "    if activation == 'sigmoid':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = activation_sigmoid(Z)\n",
    "        cache = (A_prev, W, b, A)\n",
    "        return A, cache\n",
    "    \n",
    "    elif activation == 'tanh':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = activation_tanh(Z)\n",
    "        cache = (A_prev, W, b, A)\n",
    "        return A, cache\n",
    "    \n",
    "    elif activation == 'relu':\n",
    "        #print('b.shape = ' + str(b.shape))\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        #print('计算后Z.shape = ' + str(Z.shape))\n",
    "        A = activation_relu(Z)\n",
    "        #print('计算后A.shape = ' + str(A.shape))\n",
    "        cache = (A_prev, W, b, A)\n",
    "        return A, cache\n",
    "    elif activation == 'softmax':\n",
    "        Z = np.dot(W, A_prev) + b\n",
    "        A = softmax(Z)\n",
    "        cache = (A_prev, W, b, A)\n",
    "    \n",
    "    else:\n",
    "        print('error')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters, activation):\n",
    "    caches = []\n",
    "    L = len(parameters) // 2\n",
    "    A = X\n",
    "       \n",
    "    if activation == 'linear':\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = linear_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)])\n",
    "            caches.append(cache)\n",
    "        \n",
    "    if activation == 'sigmoid':\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = activation_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],activation = 'sigmoid')\n",
    "            caches.append(cache)\n",
    "        \n",
    "    if activation == 'tanh':\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = activation_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],activation = 'tanh')\n",
    "            caches.append(cache)\n",
    "        \n",
    "    if activation == 'relu':\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            A, cache = activation_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)],activation = 'relu')\n",
    "            caches.append(cache)\n",
    "    # 最后一层\n",
    "    A_prev = A\n",
    "    A, _ = activation_prop(A_prev, parameters['W'+str(L)], parameters['b' + str(L)], activation = 'relu')\n",
    "    \n",
    "    # softmax\n",
    "    AL = softmax(A)\n",
    "    cache = (A_prev, parameters['W'+str(L)], parameters['b' + str(L)], AL)\n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (10, X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y, AL):\n",
    "    # m为实例个数\n",
    "    m = Y.shape[1]\n",
    "    # cost为交叉熵\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL + 1e-8)) + np.multiply(1 - Y, np.log(1 - AL + 1e-8)))\n",
    "    #cost = 1 / m * np.sum((AL-Y)**2)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    '''\n",
    "    输入Z的梯度\n",
    "    cache保存了当初计算Z时用的A_prev,W,b\n",
    "    返回上层计算Z时用的A,W,b的梯度\n",
    "    '''\n",
    "    A_prev, W, b, A = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "    db = 1 / m * np.squeeze(np.sum(dZ, axis=1, keepdims=True))\n",
    "    db = np.reshape(db,(db.shape[0],1))\n",
    "    #print('一次反向传播后db.shape = ' + str(db.shape))\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    A_prev, W, b, A = cache\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backprop(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    输入当前层激活后的函数值与相应的激活函数种类，\n",
    "    计算激活函数的导数，\n",
    "    乘上dA后得到dZ，再将dZ交给linear_backward计算，\n",
    "    返回上层计算Z时用的A,W,b的梯度\n",
    "    \"\"\"\n",
    "    A_prev, W, b, A = cache\n",
    "    if activation == \"relu\":\n",
    "        #dZ = dA * (A > 0)\n",
    "        dZ = relu_backward(dA, cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = dA * (A * (1 - A))\n",
    "        dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "        \n",
    "    elif activation =='tanh':\n",
    "        dZ = dA * (1 - np.power(A,2))\n",
    "        dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "        \n",
    "    elif activation =='linear':\n",
    "        dA_prev, dW, db = linear_backward(dZ, cache)\n",
    "        \n",
    "    else:\n",
    "        print('error')\n",
    "        return None\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layers_backprop(AL, Y, caches, activation='relu'):\n",
    "    \"\"\"\n",
    "    含有3层神经网络的反向传播\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = 3\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = -( Y / AL ) + (1 - Y) / (1 - AL)\n",
    "    dZL = AL - Y\n",
    "    \n",
    "    A2, W3, b3 = cache[-1]\n",
    "    A1, W2, b2 = cache[-2]\n",
    "    A0, W1, b1 = cache[-3]\n",
    "    \n",
    "    m = A0.shape[1]\n",
    "\n",
    "    dW3 = 1 / m * np.dot(dZL, A2.T)\n",
    "    db3 = 1 / m * np.squeeze(np.sum(dZL, axis=1, keepdims=True))\n",
    "    dA2 = np.dot(W3.T, dZL)\n",
    "    dZ2 = dA2 * (A2 > 0)\n",
    "    \n",
    "    dW2 = 1 / m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1 / m * np.squeeze(np.sum(dZ2, axis=1, keepdims=True))\n",
    "    dA1 = np.dot(W3.T, dZL)\n",
    "    dZ1 = dA1 * (A1 > 0)\n",
    "    \n",
    "    dW1 = 1 / m * np.dot(dZ1, A0.T)\n",
    "    db1 = 1 / m * np.squeeze(np.sum(dZ1, axis=1, keepdims=True))\n",
    "    assert(dW1.shape == W1.shape)\n",
    "    assert(dW2.shape == W2.shape)\n",
    "    assert(dW3.shape == W3.shape)\n",
    "    assert(db1.shape == b1.shape)\n",
    "    assert(db2.shape == b2.shape)\n",
    "    assert(db3.shape == b2.shape)\n",
    "    \n",
    "    return dW1,dW2,dW3,db1,db2,db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layers_backprop(AL, Y, caches, activation):\n",
    "    \"\"\"\n",
    "    含有L层神经网络的反向传播\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    # 最后一层激活函数为softmax\n",
    "    dAL = -( Y / AL ) + (1 - Y) / (1 - AL)\n",
    "    dZL = AL - Y\n",
    "    # 获得计算最后一层的cache\n",
    "    current_cache = caches[-1]\n",
    "    # 获得计算最后一层的W，b的梯度\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(dZL,current_cache)\n",
    "    grads[\"dA\" + str(L)] = dAL\n",
    "    for l in [2,1]:\n",
    "        # 从倒数第二层开始，即 L-1层,\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backprop(grads[\"dA\"+str(l)], caches[l-1], activation='relu')\n",
    "        grads[\"dA\" + str(l-1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l)] = dW_temp\n",
    "        grads[\"db\" + str(l)] = db_temp\n",
    "        #print('grads[\"dA\"{}'.format(l) + str(grads[\"dA\" + str(l)].shape))\n",
    "    #print(grads.keys())    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n"
     ]
    }
   ],
   "source": [
    "i = [l for l in reversed(range(1,3))]\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "print([l for l in reversed(range(1,10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    梯度下降,更新参数\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2\n",
    "    #print('L(grads)//2 = ' + str(len(grads) // 2) )\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layers_nn(X,Y,layers_dimentions,learning_rate=0.03,num_iterations=5001,activation='relu'):\n",
    "    \"\"\"\n",
    "    三层神经网络的模型，\n",
    "    输入训练集X，标签集Y，三层网络分别的节点数\n",
    "    默认学习率0.01，训练5000次，激活函数为relu\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    caches = []\n",
    "    m = X.shape[1] \n",
    "    \n",
    "    parameters = initialize_parameters_n(layers_dimentions)\n",
    "    \"\"\"W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\"\"\"\n",
    "    \n",
    "    '''print('主函数中W1.shape = ' + str(W1.shape))\n",
    "    print('主函数中W2.shape = ' + str(W2.shape))\n",
    "    print('主函数中W3.shape = ' + str(W3.shape))\n",
    "    print('主函数中b1.shape = ' + str(b1.shape))\n",
    "    print('主函数中b2.shape = ' + str(b2.shape))\n",
    "    print('主函数中b3.shape = ' + str(b3.shape))'''\n",
    "    for i in range(0, num_iterations):\n",
    "        # 前向传播\n",
    "        learning_rate_decay = learning_rate * np.power(0.99,i//100)\n",
    "        A3, caches = forward_prop(X, parameters, activation)\n",
    "        \"\"\"\n",
    "        A0 = X\n",
    "        A1, cache1 = activation_prop(A0, W1, b1, activation=activation)\n",
    "        A2, cache2 = activation_prop(A1, W2, b2, activation=activation)\n",
    "        A3, cache3 = activation_prop(A2, W3, b3, activation=activation)\n",
    "        caches.append(cache1)\n",
    "        caches.append(cache2)\n",
    "        caches.append(cache3)\n",
    "        \"\"\"\n",
    "        # 计算ocst\n",
    "        cost = compute_cost(Y, A3)\n",
    "        \n",
    "        # 反向传播\n",
    "        #dA3 = - (np.divide(Y, A3+0.0001) - np.divide(1 - Y, (1 - A3)+0.0001))\n",
    "        #dZ3 = A3 - Y\n",
    "        grads = L_layers_backprop(A3, Y, caches, activation)\n",
    "        \n",
    "        \"\"\"dA2, dW3, db3 = activation_backprop(dA3, caches[-1], activation='relu')\n",
    "        dA1, dW2, db2 = activation_backprop(dA2, caches[-2], activation='relu')\n",
    "        _,dW1,db1 = activation_backprop(dA1,caches[-3],activation='relu')\"\"\"\n",
    "        \n",
    "        \"\"\"# 记录梯度\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        grads['dW3'] = dW3\n",
    "        grads['db3'] = db3\"\"\"\n",
    "        \n",
    "        # 更新参数\n",
    "        parameters = update_parameters(parameters, grads, learning_rate=learning_rate_decay)\n",
    "\n",
    "        \"\"\"# 将参数记录进字典parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        W3 = parameters[\"W3\"]\n",
    "        b3 = parameters[\"b3\"]\"\"\"\n",
    "        #print('一次迭代后b1.shape = ' + str(b1.shape))\n",
    "        # 每100次打印并记录cost值\n",
    "        if i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            costs.append(cost)\n",
    "       \n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.ylabel('cost')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['batch_label', 'labels', 'data', 'filenames'])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def load_file(cifar):\n",
    "    '''加载cifar数据集'''\n",
    "    with open('D:\\cifar-10-batches-py\\\\'+str(cifar), 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='latin1')\n",
    "    return data\n",
    "data = load_file('test_batch')\n",
    "print(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing batch 1 of 1\n"
     ]
    }
   ],
   "source": [
    "print(data['batch_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_x = data['data'] \n",
    "data_all_x = data_all_x.T      # shape = (3072, 10000)\n",
    "data_all_y = data['labels']\n",
    "\n",
    "data_all_y = np.array(data_all_y)\n",
    "#print(data1_y.shape)\n",
    "\n",
    "data_all_y = tf.one_hot(data_all_y, 10, axis = 0)\n",
    "sess = tf.Session()\n",
    "data_all_y = sess.run(data_all_y) # shape = (10, 10000)\n",
    "\n",
    "data1_x = data_all_x[:,0:500]\n",
    "data1_y = data_all_y[:,0:500]\n",
    "\n",
    "data_x_test = data_all_x[:,500:]\n",
    "data_y_test = data_all_y[:,500:]\n",
    "data_y_test = np.array(data_y_test)\n",
    "\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dimentions = [3072,20,15,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action = \"ignore\", category = RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.25110021697056\n",
      "Cost after iteration 100: 3.23799221134182\n",
      "Cost after iteration 200: 3.2009933899111767\n",
      "Cost after iteration 300: 3.11673778691367\n",
      "Cost after iteration 400: 3.0275136683139863\n",
      "Cost after iteration 500: 2.980875065850043\n",
      "Cost after iteration 600: 2.9407950114663355\n",
      "Cost after iteration 700: 2.948517452015645\n",
      "Cost after iteration 800: 2.895761428285215\n",
      "Cost after iteration 900: 2.8350432614060552\n",
      "Cost after iteration 1000: 2.7608594044053536\n",
      "Cost after iteration 1100: 2.68144119190527\n",
      "Cost after iteration 1200: 2.5902195688845944\n",
      "Cost after iteration 1300: 2.5068160043609526\n",
      "Cost after iteration 1400: 2.5028717568110967\n",
      "Cost after iteration 1500: 2.4863152880706676\n",
      "Cost after iteration 1600: 2.2191525093332407\n",
      "Cost after iteration 1700: 2.5251901877318814\n",
      "Cost after iteration 1800: 2.2079747743989673\n",
      "Cost after iteration 1900: 2.2461805148675005\n",
      "Cost after iteration 2000: 2.026579421638606\n",
      "Cost after iteration 2100: 2.292096531588587\n",
      "Cost after iteration 2200: 1.9835399549182067\n",
      "Cost after iteration 2300: 1.7493690044669157\n",
      "Cost after iteration 2400: 1.706748565970703\n",
      "Cost after iteration 2500: 1.7370355727338065\n",
      "Cost after iteration 2600: 1.7655816693648183\n",
      "Cost after iteration 2700: 1.430051404185142\n",
      "Cost after iteration 2800: 1.3990270484333296\n",
      "Cost after iteration 2900: 1.669488139748074\n",
      "Cost after iteration 3000: 1.7329057906630307\n",
      "Cost after iteration 3100: 1.0541140623679306\n",
      "Cost after iteration 3200: 1.004271226387645\n",
      "Cost after iteration 3300: 1.1261191555691297\n",
      "Cost after iteration 3400: 0.8163942191496806\n",
      "Cost after iteration 3500: 0.7538596107686988\n",
      "Cost after iteration 3600: 0.8445111929320588\n",
      "Cost after iteration 3700: 0.6323803172697852\n",
      "Cost after iteration 3800: 0.8356722995221931\n",
      "Cost after iteration 3900: 0.5410214241852711\n",
      "Cost after iteration 4000: 0.5211552887645112\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "para = three_layers_nn(data1_x,data1_y,layers_dimentions=layers_dimentions,learning_rate=0.0006,num_iterations=4001,activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(tf.one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(parameters,X_test,Y_test):\n",
    "    m = X_test.shape[1]\n",
    "    Y_pred, _ = forward_prop(X_test, parameters, activation='relu')\n",
    "    sess = tf.Session()\n",
    "    errors = tf.reduce_mean((Y_pred - Y_test))\n",
    "    \n",
    "    print(sess.run(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0665479373211062e-17\n"
     ]
    }
   ],
   "source": [
    "errors(para,data_x_test,data_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_para(para):\n",
    "    \n",
    "    W1 = tf.Variable(para['W1'], dtype = tf.float32, name = 'W1')\n",
    "    W2 = tf.Variable(para['W2'], dtype = tf.float32, name = 'W2')\n",
    "    W3 = tf.Variable(para['W3'], dtype = tf.float32, name = 'W3')\n",
    "    b1 = tf.Variable(para['b1'], dtype = tf.float32, name = 'b1')\n",
    "    b2 = tf.Variable(para['b2'], dtype = tf.float32, name = 'b2')\n",
    "    b3 = tf.Variable(para['b3'], dtype = tf.float32, name = 'b3')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        save_path = saver.save(sess, 'D:\\cifar-10-batches-py\\save_para_with_sigmoid.ckpt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_para(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
